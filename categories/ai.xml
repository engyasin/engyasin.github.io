<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Eng Yasin's Blog :) (Posts about ai)</title><link>https://engyasin.github.io/</link><description></description><atom:link href="https://engyasin.github.io/categories/ai.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2024 &lt;a href="mailto:yy33@tu-clausthal.de"&gt;Yasin Yousif&lt;/a&gt; </copyright><lastBuildDate>Sun, 19 May 2024 11:44:52 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><link>https://engyasin.github.io/posts/why-the-new-kolmogorov-arnold-networks-so-promising/</link><dc:creator>Yasin Yousif (llama3-comments)</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;em&gt;Recently, (yet) another new neural network structure was proposed. Namely, Kolmogorov-Arnold Network (KAN). Soon this new structure attracted a lot of attention, and for good reason: interpretability. For what current Multi Layers Preceptron (MLPs) networks lack is a way to make sense of the network predictions. Magic isn't involved; we need to know how the learning is done, so we can improve, fix, or extend it in an efficient manner. KANs take a significant step forward in this regard using addition operators, which have been proven to represent higher-order functions effectively.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;llama3&lt;/strong&gt;: &lt;em&gt;"How does the addition operator in KANs compare to other methods for representing high-order functions, such as Fourier series or wavelet expansions? Can we expect similar benefits in terms of interpretability?"&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://engyasin.github.io/posts/why-the-new-kolmogorov-arnold-networks-so-promising/"&gt;Read more…&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>additve-models</category><category>ai</category><category>deep-learning</category><category>interpretability</category><guid>https://engyasin.github.io/posts/why-the-new-kolmogorov-arnold-networks-so-promising/</guid><pubDate>Sun, 19 May 2024 07:25:52 GMT</pubDate></item><item><link>https://engyasin.github.io/posts/why-deep-learning-sucks/</link><dc:creator>Yasin Yousif</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;em&gt;After spending some years studying and using deep learning, I always suffered from the difficulty of debugging errors, or setting hyperparameters. As a researcher this can not only waste additional time, but also money and resources. In this article, we will demonstrate how traditional rule-based methods have a hidden edge (beside simplicity) in solving complex problems that require automation.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://engyasin.github.io/posts/why-deep-learning-sucks/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>ai</category><category>deep-learning</category><category>opinion</category><guid>https://engyasin.github.io/posts/why-deep-learning-sucks/</guid><pubDate>Tue, 09 Jan 2024 11:49:14 GMT</pubDate></item><item><title>The unexpected winter of Artifical Intelligence</title><link>https://engyasin.github.io/posts/the-unexpected-winter-of-artifical-intelligence/</link><dc:creator>Yasin Yousif</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;em&gt;Nowadays, everyone is exicted the latest trends in AI applications, like ChatGPT, self-driving cars, Image synthesis, etc. This overhype is not new, it happened before in the first AI winter in the 1980s. Some warn againt it, because it may cause dispointment and even a new AI winter. But here I will talk about the bottelneck of AI research that I come across in my work. It may be not be called a winter, but it's difinitely will cause a slow down in the field.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://engyasin.github.io/posts/the-unexpected-winter-of-artifical-intelligence/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>ai</category><category>deep-learning</category><category>opinion</category><guid>https://engyasin.github.io/posts/the-unexpected-winter-of-artifical-intelligence/</guid><pubDate>Sun, 05 Mar 2023 06:13:36 GMT</pubDate></item></channel></rss>