<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Robot Learning by Example (Posts about deep-learning)</title><link>https://engyasin.github.io/</link><description></description><atom:link href="https://engyasin.github.io/categories/deep-learning.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2025 &lt;a href="mailto:yy33@tu-clausthal.de"&gt;Yasin Yousif&lt;/a&gt; </copyright><lastBuildDate>Tue, 11 Feb 2025 08:51:46 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><link>https://engyasin.github.io/posts/hands-on-imitation-learning/</link><dc:creator>Yasin Yousif</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;em&gt;An overview of the most prominent imitation learning methods with tests on a grid environment&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Reinforcement learning is one branch of machine learning concerned with learning by guidance of scalar signals (rewards); in contrast to supervised learning, which needs full labels of  the target variable.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://engyasin.github.io/posts/hands-on-imitation-learning/"&gt;Read more…&lt;/a&gt; (18 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>deep-learning</category><category>reinforcement learning</category><category>review</category><guid>https://engyasin.github.io/posts/hands-on-imitation-learning/</guid><pubDate>Sat, 07 Sep 2024 22:53:52 GMT</pubDate></item><item><link>https://engyasin.github.io/posts/why-the-new-kolmogorov-arnold-networks-so-promising/</link><dc:creator>Yasin Yousif (llama3-comments)</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;em&gt;Recently, (yet) another new neural network structure was proposed. Namely, Kolmogorov-Arnold Network (KAN). Soon this new structure attracted a lot of attention, and for good reason: interpretability. For what current Multi Layers Preceptron (MLPs) networks lack is a way to make sense of the network predictions. Magic isn't involved; we need to know how the learning is done, so we can improve, fix, or extend it in an efficient manner. KANs take a significant step forward in this regard using addition operators, which have been proven to represent higher-order functions effectively.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://engyasin.github.io/posts/why-the-new-kolmogorov-arnold-networks-so-promising/"&gt;Read more…&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>additve-models</category><category>ai</category><category>deep-learning</category><category>interpretability</category><guid>https://engyasin.github.io/posts/why-the-new-kolmogorov-arnold-networks-so-promising/</guid><pubDate>Sun, 19 May 2024 07:25:52 GMT</pubDate></item><item><link>https://engyasin.github.io/posts/why-deep-learning-sucks/</link><dc:creator>Yasin Yousif</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;em&gt;After spending some years studying and using deep learning, I always suffered from the difficulty of debugging errors, or setting hyperparameters. As a researcher this can not only waste additional time, but also money and resources. In this article, we will demonstrate how traditional rule-based methods have a hidden edge (beside simplicity) in solving complex problems that require automation.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://engyasin.github.io/posts/why-deep-learning-sucks/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>ai</category><category>deep-learning</category><category>opinion</category><guid>https://engyasin.github.io/posts/why-deep-learning-sucks/</guid><pubDate>Tue, 09 Jan 2024 11:49:14 GMT</pubDate></item><item><title>The unexpected winter of Artifical Intelligence</title><link>https://engyasin.github.io/posts/the-unexpected-winter-of-artifical-intelligence/</link><dc:creator>Yasin Yousif</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;em&gt;Nowadays, everyone is exicted the latest trends in AI applications, like ChatGPT, self-driving cars, Image synthesis, etc. This overhype is not new, it happened before in the first AI winter in the 1980s. Some warn againt it, because it may cause dispointment and even a new AI winter. But here I will talk about the bottelneck of AI research that I come across in my work. It may be not be called a winter, but it's difinitely will cause a slow down in the field.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://engyasin.github.io/posts/the-unexpected-winter-of-artifical-intelligence/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>ai</category><category>deep-learning</category><category>opinion</category><guid>https://engyasin.github.io/posts/the-unexpected-winter-of-artifical-intelligence/</guid><pubDate>Sun, 05 Mar 2023 06:13:36 GMT</pubDate></item><item><link>https://engyasin.github.io/posts/train-your-deep-neural-network-faster-with-automatic-mixed-precision/</link><dc:creator>Yasin Yousif</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;em&gt;Have you been working on deep learning model with big size and wandered how to squeeze every possibility to save your time? or maybe you have the best GPU hardware but still find the speed too slow. Well, look at the bright side. This means you still have room for improvment :)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://engyasin.github.io/posts/train-your-deep-neural-network-faster-with-automatic-mixed-precision/"&gt;Read more…&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>deep-learning</category><category>pytorch</category><category>tips</category><guid>https://engyasin.github.io/posts/train-your-deep-neural-network-faster-with-automatic-mixed-precision/</guid><pubDate>Fri, 23 Sep 2022 14:53:19 GMT</pubDate></item></channel></rss>