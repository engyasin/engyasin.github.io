<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Robot Learning by Example (Posts about MujoCo)</title><link>https://engyasin.github.io/</link><description></description><atom:link href="https://engyasin.github.io/categories/mujoco.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2025 &lt;a href="mailto:yy33@tu-clausthal.de"&gt;Yasin Yousif&lt;/a&gt; </copyright><lastBuildDate>Wed, 25 Jun 2025 11:06:00 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><link>https://engyasin.github.io/posts/the-reinforcement-learning-algorithmic-landscape/</link><dc:creator>Yasin Yousif</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;em&gt;Deep Reinforcement Learning is a branch of machine learning, gaining rapid traction recently as an efficient approach to skills acquisition by machines, across diverse fields of applications. From optimizing advertising placement to enabling robotic manipulation and through refining Large Language Models responses, its potential is predicted to be immense. The field encompasses a rich variety of algorithms, spanning model-based and model-free techniques, policy gradients, and Q-learning methods. This dynamic landscape can be daunting for newcomers, even those with a foundation in supervised learning. In this post we provide a comprehensive overview, offering both a high-level perspective as well as a detailed examination of the underlying mathematical derivations and algorithms specifics. We’re also including full benchmarking results for all methods in both discrete and continuous environments, along with some unsolved questions and recommended readings for the interested reader.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://engyasin.github.io/posts/the-reinforcement-learning-algorithmic-landscape/"&gt;Read more…&lt;/a&gt; (36 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>benchmarking</category><category>guide</category><category>MujoCo</category><category>reinforcement learning</category><category>tutorial</category><guid>https://engyasin.github.io/posts/the-reinforcement-learning-algorithmic-landscape/</guid><pubDate>Sun, 11 May 2025 16:34:24 GMT</pubDate></item><item><link>https://engyasin.github.io/posts/immerse-yourself-in-reinforcement-learning-and-robotics-with-mujoco/</link><dc:creator>Yasin Yousif</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;em&gt;MujoCo is a physics simulator for robotics research developed by Google DeepMind and written in C++ with a Python API. The advantage of using MujoCo is due to its various implemented models along with full dynamic and physics properties, such as friction, inertia, elasticity, etc. This realism allows researchers to rigorously test reinforcement learning algorithms in simulations before deployment, mitigating risks associated with real-world applications. Simulating exact replicas of robot manipulators becomes particularly valuable, enabling training in a safe virtual environment and seamless transition to production. Notable examples include open-source models for popular brands like ALOHA, FRANKA, and KUKA readily available within MujoCo.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://engyasin.github.io/posts/immerse-yourself-in-reinforcement-learning-and-robotics-with-mujoco/"&gt;Read more…&lt;/a&gt; (12 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>MujoCo</category><category>reinforcement learning</category><category>robotics</category><category>simulation</category><category>tutorial</category><guid>https://engyasin.github.io/posts/immerse-yourself-in-reinforcement-learning-and-robotics-with-mujoco/</guid><pubDate>Wed, 05 Feb 2025 19:10:29 GMT</pubDate></item></channel></rss>